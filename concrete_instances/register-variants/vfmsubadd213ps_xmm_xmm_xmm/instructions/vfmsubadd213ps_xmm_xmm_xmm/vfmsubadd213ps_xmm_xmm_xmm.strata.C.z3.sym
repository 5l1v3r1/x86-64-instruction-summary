code: vfmsubadd213ps %xmm3, %xmm2, %xmm1

  maybe read:      { %xmm1 %xmm2 %xmm3 }
  must read:       { %xmm1 %xmm2 %xmm3 }
  maybe write:     { %ymm1 }
  must write:      { %ymm1 }
  maybe undef:     { }
  must undef:      { }
  required flags:  { fma }

vfmsubadd213ps %xmm3, %xmm2, %xmm1: Hindex8(9)
vfmsubadd213ps %xmm3, %xmm2, %xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm10_xmm11: Hindex8(9)
callq .move_byte_11_of_ymm1_to_r9b: Hindex8(9)
callq .move_r9b_to_byte_24_of_ymm1: Hindex8(9)
callq .move_byte_6_of_ymm1_to_r9b: Hindex8(9)
callq .move_r9b_to_byte_16_of_ymm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_r8b_to_byte_0_of_ymm1: Hindex8(9)
xorq %rdx, %rdx: Hindex1(9)
adcb %bl, %dl: Hindex2(9)
movslq %ebx, %rax: Hindex5(9)
movq $0xffffffffffffffff, %rbx: Hindex5(9)
movswq %bx, %r10: Hindex5(9)
movslq %ebx, %rbx: Hindex5(9)
callq .move_032_016_ecx_r8w_r9w: Hindex8(9)
callq .move_064_032_rbx_r10d_r11d: Hindex8(9)
movq %r10, %rcx: Hindex5(9)
callq .move_016_032_r8w_r9w_ebx: Hindex8(9)
xorq %rcx, %rbx: Hindex1(9)
callq .set_szp_for_ebx: Hindex8(9)
movq $0x20, %rbx: Hindex5(9)
callq .move_016_008_cx_r12b_r13b: Hindex8(9)
callq .move_008_016_r12b_r13b_bx: Hindex8(9)
callq .move_064_032_rbx_r8d_r9d: Hindex8(9)
movq $0xffffffffffffffc0, %rbx: Hindex5(9)
movswq %cx, %rbx: Hindex5(9)
movswq %bx, %r9: Hindex5(9)
movswq %bx, %r10: Hindex5(9)
movslq %ebx, %rbx: Hindex5(9)
callq .move_032_016_ecx_r8w_r9w: Hindex8(9)
callq .move_064_032_rbx_r10d_r11d: Hindex8(9)
movq %r10, %rcx: Hindex5(9)
callq .move_016_032_r8w_r9w_ebx: Hindex8(9)
callq .move_016_032_r8w_r9w_edx: Hindex8(9)
callq .move_016_008_dx_r8b_r9b: Hindex8(9)
callq .move_r9b_to_byte_8_of_ymm1: Hindex8(9)
callq .move_128_64_xmm2_xmm8_xmm9: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
movq %r12, %r13: Hindex5(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_256_128_ymm3_xmm8_xmm9: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r10_r11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
orq %r9, %r11: Hindex1(9)
orq %r8, %r10: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_256_128_ymm2_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm3_r10_r11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
orq %r9, %r11: Hindex1(9)
orq %r8, %r10: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_256_128_ymm3_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_64_xmm1_xmm12_xmm13: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
movq %r10, %rbx: Hindex5(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_256_128_ymm2_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r10, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm12_xmm13: Hindex8(9)
callq .move_128_64_xmm1_xmm12_xmm13: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
movq %r10, %rbx: Hindex5(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r10, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
movswq %bx, %r10: Hindex5(9)
movslq %ebx, %rbx: Hindex5(9)
callq .move_032_016_ecx_r8w_r9w: Hindex8(9)
callq .move_064_032_rbx_r10d_r11d: Hindex8(9)
movq %r10, %rcx: Hindex5(9)
callq .move_016_032_r8w_r9w_ebx: Hindex8(9)
xorq %rcx, %rbx: Hindex1(9)
callq .set_szp_for_ebx: Hindex8(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r10, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
vminpd %ymm2, %ymm2, %ymm1: Hindex0(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
movq %r10, %rbx: Hindex5(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r10_r11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
orq %r9, %r11: Hindex1(9)
orq %r8, %r10: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
vmaxps %ymm2, %ymm2, %ymm10: Hindex0(9)
vmaxpd %ymm10, %ymm2, %ymm1: Hindex0(9)
callq .move_128_256_xmm10_xmm11_ymm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r10, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm12_xmm13: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_032_xmm1_xmm4_xmm5_xmm6_xmm7: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
vmaxps %ymm3, %ymm11, %ymm1: Hindex0(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_byte_5_of_ymm1_to_r9b: Hindex8(9)
callq .move_064_128_r8_r9_xmm2: Hindex8(9)
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_64_128_xmm8_xmm9_xmm1: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
vrcpps %ymm1, %ymm10: Hindex0(9)
callq .move_128_256_xmm10_xmm11_ymm1: Hindex8(9)
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm12_xmm13: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
vrcpps %ymm1, %ymm10: Hindex0(9)
callq .move_128_256_xmm10_xmm11_ymm1: Hindex8(9)
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_64_128_xmm8_xmm9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
vmulpd %ymm8, %ymm3, %ymm6: Hindex0(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_128_064_xmm3_r12_r13: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
xorq %r13, %r9: Hindex1(9)
xorq %r12, %r8: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm10_xmm11: Hindex8(9)
callq .move_64_128_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_256_128_ymm2_xmm8_xmm9: Hindex8(9)
callq .move_256_128_ymm3_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm12_xmm13: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
movq %r10, %rbx: Hindex5(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm12_xmm13: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
movq %r10, %rbx: Hindex5(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_64_128_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
vaddps %ymm10, %ymm11, %ymm3: Hindex0(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7: Hindex8(9)
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1: Hindex8(9)
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7: Hindex8(9)
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1: Hindex8(9)
callq .move_256_128_ymm3_xmm8_xmm9: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r10, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm12_xmm13: Hindex8(9)
callq .move_128_64_xmm1_xmm12_xmm13: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
movq %r10, %rbx: Hindex5(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r10, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
movswq %bx, %r10: Hindex5(9)
movslq %ebx, %rbx: Hindex5(9)
callq .move_032_016_ecx_r8w_r9w: Hindex8(9)
callq .move_064_032_rbx_r10d_r11d: Hindex8(9)
movq %r10, %rcx: Hindex5(9)
callq .move_016_032_r8w_r9w_ebx: Hindex8(9)
xorq %rcx, %rbx: Hindex1(9)
callq .set_szp_for_ebx: Hindex8(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r10, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
vminpd %ymm2, %ymm2, %ymm1: Hindex0(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
movq %r10, %rbx: Hindex5(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r10_r11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
orq %r9, %r11: Hindex1(9)
orq %r8, %r10: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
vmaxps %ymm2, %ymm2, %ymm10: Hindex0(9)
vmaxpd %ymm10, %ymm2, %ymm1: Hindex0(9)
callq .move_128_256_xmm10_xmm11_ymm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r10, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm12_xmm13: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_032_xmm1_xmm4_xmm5_xmm6_xmm7: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
vmaxps %ymm3, %ymm11, %ymm1: Hindex0(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_byte_5_of_ymm1_to_r9b: Hindex8(9)
callq .move_064_128_r8_r9_xmm2: Hindex8(9)
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_64_128_xmm8_xmm9_xmm1: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
vrcpps %ymm1, %ymm10: Hindex0(9)
callq .move_128_256_xmm10_xmm11_ymm1: Hindex8(9)
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm12_xmm13: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
vrcpps %ymm1, %ymm10: Hindex0(9)
callq .move_128_256_xmm10_xmm11_ymm1: Hindex8(9)
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_64_128_xmm8_xmm9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
vmulpd %ymm8, %ymm3, %ymm6: Hindex0(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_128_064_xmm3_r12_r13: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
xorq %r13, %r9: Hindex1(9)
xorq %r12, %r8: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm10_xmm11: Hindex8(9)
callq .move_64_128_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_256_128_ymm2_xmm8_xmm9: Hindex8(9)
callq .move_256_128_ymm3_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm12_xmm13: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
movq %r10, %rbx: Hindex5(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm12_xmm13: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
movq %r10, %rbx: Hindex5(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_64_128_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
vaddps %ymm10, %ymm11, %ymm3: Hindex0(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7: Hindex8(9)
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1: Hindex8(9)
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7: Hindex8(9)
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1: Hindex8(9)
callq .move_128_256_xmm10_xmm11_ymm1: Hindex8(9)
callq .move_256_128_ymm2_xmm8_xmm9: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
vfnmsub132ps %ymm1, %ymm13, %ymm6: Hindex1(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r10_r11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
orq %r9, %r11: Hindex1(9)
orq %r8, %r10: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm1_r8_r9: Hindex8(9)
xorq %r8, %r12: Hindex1(9)
xorq %r9, %r13: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
vfnmsub132ps %ymm3, %ymm3, %ymm3: Hindex1(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
vaddps %ymm10, %ymm11, %ymm3: Hindex0(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_032_xmm3_xmm4_xmm5_xmm6_xmm7: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
vminps %ymm1, %ymm14, %ymm1: Hindex0(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
vminps %ymm1, %ymm14, %ymm1: Hindex0(9)
vsubps %ymm10, %ymm4, %ymm2: Hindex0(9)
callq .move_128_64_xmm2_xmm10_xmm11: Hindex8(9)
callq .move_64_128_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_256_128_ymm2_xmm8_xmm9: Hindex8(9)
callq .move_256_128_ymm3_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm12_xmm13: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
movq %r10, %rbx: Hindex5(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm12_xmm13: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_128_032_xmm1_r10d_r11d_r12d_r13d: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
movq %r10, %rbx: Hindex5(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_032_xmm3_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
vrcpps %ymm1, %ymm10: Hindex0(9)
callq .move_128_256_xmm10_xmm11_ymm1: Hindex8(9)
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r12_r13: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
xorq %r13, %r9: Hindex1(9)
xorq %r12, %r8: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm1_r8_r9: Hindex8(9)
xorq %r8, %r12: Hindex1(9)
xorq %r9, %r13: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r10_r11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
orq %r9, %r11: Hindex1(9)
orq %r8, %r10: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r12_r13: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
xorq %r13, %r9: Hindex1(9)
xorq %r12, %r8: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
vfnmsub132ps %ymm1, %ymm13, %ymm6: Hindex1(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
vfmsub132ps %ymm0, %ymm4, %ymm1: Hindex1(9)
callq .move_256_128_ymm3_xmm8_xmm9: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r10_r11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
orq %r9, %r11: Hindex1(9)
orq %r8, %r10: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_256_128_ymm2_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm3_r10_r11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
orq %r9, %r11: Hindex1(9)
orq %r8, %r10: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_256_128_ymm3_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm3_r10_r11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
orq %r9, %r11: Hindex1(9)
orq %r8, %r10: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm12_xmm13: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm3: Hindex8(9)
vmulpd %ymm14, %ymm3, %ymm12: Hindex0(9)
callq .move_64_128_xmm12_xmm13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
Formula:

%ymm1  : (let ((a!1 (vfnmsub132_single
             (add_single (concat #x000000 ((_ extract 31 24) %ymm3)) #x00000000)
             ((_ extract 127 96) %ymm3)
             ((_ extract 127 96) %ymm3)))
      (a!5 (vfnmsub132_single
             (add_single (concat #x000000 ((_ extract 23 16) %ymm3)) #x00000000)
             ((_ extract 95 64) %ymm3)
             ((_ extract 95 64) %ymm3)))
      (a!10 (vfnmsub132_single
              (add_single (concat #x000000 ((_ extract 15 8) %ymm3)) #x00000000)
              ((_ extract 63 32) %ymm3)
              ((_ extract 63 32) %ymm3)))
      (a!14 (vfnmsub132_single
              (add_single (concat #x000000 ((_ extract 7 0) %ymm3)) #x00000000)
              ((_ extract 31 0) %ymm3)
              ((_ extract 31 0) %ymm3))))
(let ((a!2 (bvor #x00000000
                 (bvxor #x00000000
                        (add_single (vfnmsub132_single
                                      #x00000000
                                      #x00000000
                                      #x00000000)
                                    a!1))))
      (a!6 (bvor #x00000000
                 (bvxor #x00000000
                        (sub_single (vfnmsub132_single
                                      #x00000000
                                      #x00000000
                                      #x00000000)
                                    a!5))))
      (a!9 (bvxor #x0000000000000000
                  (concat (add_single (vfnmsub132_single
                                        #x00000000
                                        #x00000000
                                        #x00000000)
                                      a!1)
                          (sub_single (vfnmsub132_single
                                        #x00000000
                                        #x00000000
                                        #x00000000)
                                      a!5))))
      (a!11 (bvor #x00000000
                  (bvxor #x00000000
                         (add_single (vfnmsub132_single
                                       #x00000000
                                       #x00000000
                                       #x00000000)
                                     a!10))))
      (a!15 (bvor #x00000000
                  (bvxor #x00000000
                         (sub_single (vfnmsub132_single
                                       #x00000000
                                       #x00000000
                                       #x00000000)
                                     a!14))))
      (a!18 (bvxor #x0000000000000000
                   (concat (add_single (vfnmsub132_single
                                         #x00000000
                                         #x00000000
                                         #x00000000)
                                       a!10)
                           (sub_single (vfnmsub132_single
                                         #x00000000
                                         #x00000000
                                         #x00000000)
                                       a!14)))))
(let ((a!3 (bvxor (bvxor #x00000000
                         (add_single (vfnmsub132_single
                                       #x00000000
                                       #x00000000
                                       #x00000000)
                                     a!1))
                  a!2))
      (a!7 (bvxor (bvxor #x00000000
                         (sub_single (vfnmsub132_single
                                       #x00000000
                                       #x00000000
                                       #x00000000)
                                     a!5))
                  a!6))
      (a!12 (bvxor (bvxor #x00000000
                          (add_single (vfnmsub132_single
                                        #x00000000
                                        #x00000000
                                        #x00000000)
                                      a!10))
                   a!11))
      (a!16 (bvxor (bvxor #x00000000
                          (sub_single (vfnmsub132_single
                                        #x00000000
                                        #x00000000
                                        #x00000000)
                                      a!14))
                   a!15)))
(let ((a!4 (vfmsub132_single ((_ extract 127 96) %ymm2)
                             (vfnmsub132_single
                               a!3
                               (add_single (vfnmsub132_single
                                             #x00000000
                                             #x00000000
                                             #x00000000)
                                           a!1)
                               (add_single (vfnmsub132_single
                                             #x00000000
                                             #x00000000
                                             #x00000000)
                                           a!1))
                             ((_ extract 127 96) %ymm1)))
      (a!8 (vfmsub132_single ((_ extract 95 64) %ymm2)
                             (vfnmsub132_single
                               a!7
                               (sub_single (vfnmsub132_single
                                             #x00000000
                                             #x00000000
                                             #x00000000)
                                           a!5)
                               (sub_single (vfnmsub132_single
                                             #x00000000
                                             #x00000000
                                             #x00000000)
                                           a!5))
                             ((_ extract 95 64) %ymm1)))
      (a!13 (vfmsub132_single ((_ extract 63 32) %ymm2)
                              (vfnmsub132_single
                                a!12
                                (add_single (vfnmsub132_single
                                              #x00000000
                                              #x00000000
                                              #x00000000)
                                            a!10)
                                (add_single (vfnmsub132_single
                                              #x00000000
                                              #x00000000
                                              #x00000000)
                                            a!10))
                              ((_ extract 63 32) %ymm1)))
      (a!17 (vfmsub132_single ((_ extract 31 0) %ymm2)
                              (vfnmsub132_single
                                a!16
                                (sub_single (vfnmsub132_single
                                              #x00000000
                                              #x00000000
                                              #x00000000)
                                            a!14)
                                (sub_single (vfnmsub132_single
                                              #x00000000
                                              #x00000000
                                              #x00000000)
                                            a!14))
                              ((_ extract 31 0) %ymm1))))
(let ((a!19 (concat (bvor (concat a!4 a!8)
                          (bvxor a!9 (bvor #x0000000000000000 a!9)))
                    (bvor (concat a!13 a!17)
                          (bvxor a!18 (bvor #x0000000000000000 a!18))))))
  (concat #x00000000000000000000000000000000 a!19))))))

sigfpe  : sigfpe
sigbus  : sigbus
sigsegv : sigsegv
