code: vpbroadcastb %xmm2, %ymm1

  maybe read:      { %xmm2 }
  must read:       { %xmm2 }
  maybe write:     { %ymm1 }
  must write:      { %ymm1 }
  maybe undef:     { }
  must undef:      { }
  required flags:  { avx2 }

vpbroadcastb %xmm2, %ymm1: Hindex8(9)
vpbroadcastb %xmm2, %ymm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
movq $0xfffffffffffffff8, %rbx: Hindex5(9)
movb %bl, %ch: Hindex5(9)
movb %ch, %cl: Hindex5(9)
movq $0x40, %rbx: Hindex5(9)
movb %ah, %bl: Hindex5(9)
callq .clear_cf: Hindex8(9)
callq .set_of: Hindex8(9)
callq .read_of_into_rbx: Hindex8(9)
callq .move_064_032_rbx_r10d_r11d: Hindex8(9)
movsbq %cl, %r10: Hindex5(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
callq .set_of: Hindex8(9)
callq .read_of_into_rbx: Hindex8(9)
callq .move_064_032_rbx_r10d_r11d: Hindex8(9)
movsbq %cl, %r10: Hindex5(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
movsbq %r15b, %rcx: Hindex5(9)
adcb %cl, %r13b: Hindex2(9)
movslq %r13d, %rbx: Hindex5(9)
movb %cl, %ah: Hindex5(9)
movb %ah, %cl: Hindex5(9)
callq .set_of: Hindex8(9)
callq .read_of_into_rbx: Hindex8(9)
callq .move_064_032_rbx_r10d_r11d: Hindex8(9)
movsbq %cl, %r10: Hindex5(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
movq %rbp, %rbx: Hindex5(9)
movswq %cx, %rbx: Hindex5(9)
movswq %cx, %rbx: Hindex5(9)
callq .move_064_128_r12_r13_xmm2: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_064_128_r12_r13_xmm3: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_r8b_to_byte_3_of_ymm1: Hindex8(9)
callq .move_r8b_to_byte_2_of_ymm1: Hindex8(9)
callq .move_r8b_to_byte_1_of_ymm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_064_128_r12_r13_xmm3: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_256_128_ymm1_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_064_128_r8_r9_xmm3: Hindex8(9)
vaddpd %ymm3, %ymm1, %ymm1: Hindex0(9)
callq .move_64_128_xmm10_xmm11_xmm3: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm3: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_r8b_to_byte_23_of_ymm1: Hindex8(9)
callq .move_128_64_xmm2_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
vmaxps %ymm2, %ymm2, %ymm10: Hindex0(9)
vmaxpd %ymm10, %ymm2, %ymm1: Hindex0(9)
callq .move_128_256_xmm10_xmm11_ymm1: Hindex8(9)
callq .move_byte_1_of_ymm1_to_r8b: Hindex8(9)
callq .move_byte_0_of_ymm1_to_r9b: Hindex8(9)
callq .move_r8b_to_byte_3_of_ymm1: Hindex8(9)
callq .move_r9b_to_byte_2_of_ymm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm3: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_r8b_to_byte_23_of_ymm1: Hindex8(9)
callq .move_128_64_xmm2_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
vmaxps %ymm2, %ymm2, %ymm10: Hindex0(9)
vmaxpd %ymm10, %ymm2, %ymm1: Hindex0(9)
callq .move_128_256_xmm10_xmm11_ymm1: Hindex8(9)
Formula:

%ymm1  : (let ((a!1 (concat (concat (concat ((_ extract 7 0) %ymm2)
                                   ((_ extract 7 0) %ymm2))
                           ((_ extract 7 0) %ymm2))
                   ((_ extract 7 0) %ymm2))))
(let ((a!2 (concat ((_ extract 15 0)
                     (add_double #x0000000000000000 (concat a!1 a!1)))
                   ((_ extract 15 0)
                     (add_double #x0000000000000000 (concat a!1 a!1))))))
  (concat (concat (concat a!2 a!2) (concat a!2 a!2))
          (concat (concat a!2 a!2) (concat a!2 a!2)))))

sigfpe  : sigfpe
sigbus  : sigbus
sigsegv : sigsegv
