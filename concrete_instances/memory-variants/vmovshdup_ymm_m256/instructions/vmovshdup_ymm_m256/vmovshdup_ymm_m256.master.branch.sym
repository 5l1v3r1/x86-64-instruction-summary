code: vmovshdup (%rbx), %ymm1

  maybe read:      { %rbx }
  must read:       { %rbx }
  maybe write:     { %ymm1 }
  must write:      { %ymm1 }
  maybe undef:     { }
  must undef:      { }
  required flags:  { avx }

vmovshdup (%rbx), %ymm1: Hindex8(9)
vmovshdup (%rbx), %ymm1: Hindex8(9)
callq .move_256_128_ymm2_xmm8_xmm9: Hindex8(9)
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm2_xmm12_xmm13: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm12_xmm13: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_64_128_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm2_xmm12_xmm13: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm12_xmm13: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_64_128_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_64_xmm2_xmm12_xmm13: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
callq .move_032_064_r10d_r11d_rbx: Hindex8(9)
vzeroall: Hindex1(9)
movslq %r8d, %r9: Hindex5(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm10_xmm11: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
Formula:

%ymm1  : (concat (concat (concat ((_ extract 255 224) TMP_BV_256_0)
                        ((_ extract 255 224) TMP_BV_256_0))
                (concat ((_ extract 191 160) TMP_BV_256_1)
                        ((_ extract 191 160) TMP_BV_256_1)))
        (concat (concat ((_ extract 127 96) TMP_BV_256_2)
                        ((_ extract 127 96) TMP_BV_256_2))
                (concat ((_ extract 63 32) TMP_BV_256_3)
                        ((_ extract 63 32) TMP_BV_256_3))))

Information about memory reads:
  Value TMP_BV_256_0 (32 bytes)
    was read at address %rbx.
  Value TMP_BV_256_1 (32 bytes)
    was read at address %rbx.
  Value TMP_BV_256_2 (32 bytes)
    was read at address %rbx.
  Value TMP_BV_256_3 (32 bytes)
    was read at address %rbx.

sigfpe  : sigfpe
sigbus  : sigbus
sigsegv : sigsegv
