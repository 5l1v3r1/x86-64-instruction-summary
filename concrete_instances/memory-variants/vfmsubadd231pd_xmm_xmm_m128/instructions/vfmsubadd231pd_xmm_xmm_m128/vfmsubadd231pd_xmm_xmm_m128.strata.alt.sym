code: vfmsubadd231pd (%rbx), %xmm2, %xmm1

  maybe read:      { %rbx %xmm1 %xmm2 }
  must read:       { %rbx %xmm1 %xmm2 }
  maybe write:     { %ymm1 }
  must write:      { %ymm1 }
  maybe undef:     { }
  must undef:      { }
  required flags:  { fma }

vfmsubadd231pd (%rbx), %xmm2, %xmm1: Hindex8(9)
vfmsubadd231pd (%rbx), %xmm2, %xmm1: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
vmaxps %ymm3, %ymm11, %ymm1: Hindex0(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
vminpd %ymm8, %ymm1, %ymm1: Hindex0(9)
callq .move_128_064_xmm3_r10_r11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
orq %r9, %r11: Hindex1(9)
orq %r8, %r10: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7: Hindex8(9)
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
vminps %ymm1, %ymm14, %ymm1: Hindex0(9)
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
vfnmsub132pd %ymm1, %ymm3, %ymm2: Hindex1(9)
vmaxpd %ymm2, %ymm2, %ymm1: Hindex0(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
vmaxps %ymm3, %ymm11, %ymm1: Hindex0(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r10_r11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
orq %r9, %r11: Hindex1(9)
orq %r8, %r10: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_64_xmm1_xmm8_xmm9: Hindex8(9)
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7: Hindex8(9)
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1: Hindex8(9)
callq .move_64_128_xmm8_xmm9_xmm1: Hindex8(9)
callq .move_128_064_xmm1_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
vfmadd132ps %ymm1, %ymm3, %ymm2: Hindex1(9)
vmaxpd %ymm2, %ymm2, %ymm1: Hindex0(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
vrcpps %ymm1, %ymm10: Hindex0(9)
callq .move_128_256_xmm10_xmm11_ymm1: Hindex8(9)
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_byte_5_of_ymm1_to_r9b: Hindex8(9)
callq .move_064_128_r8_r9_xmm2: Hindex8(9)
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_64_128_xmm8_xmm9_xmm1: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm1_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm8_xmm9: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_032_xmm1_xmm4_xmm5_xmm6_xmm7: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
vmaxps %ymm3, %ymm11, %ymm1: Hindex0(9)
callq .move_128_032_xmm1_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_byte_5_of_ymm1_to_r9b: Hindex8(9)
callq .move_064_128_r8_r9_xmm2: Hindex8(9)
callq .move_128_032_xmm2_xmm8_xmm9_xmm10_xmm11: Hindex8(9)
callq .move_64_128_xmm8_xmm9_xmm1: Hindex8(9)
callq .move_032_128_xmm8_xmm9_xmm10_xmm11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
vfmsub132pd %ymm3, %ymm1, %ymm2: Hindex1(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vminpd %ymm2, %ymm2, %ymm1: Hindex0(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_032_xmm2_xmm4_xmm5_xmm6_xmm7: Hindex8(9)
callq .move_032_128_xmm4_xmm5_xmm6_xmm7_xmm1: Hindex8(9)
callq .move_128_64_xmm2_xmm8_xmm9: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall: Hindex1(9)
movq %r8, %r11: Hindex5(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
Formula:

%ymm1  : (let ((a!1 (vfmsub132_double ((_ extract 63 0) %ymm2)
                             (vfnmsub132_double
                               ((_ extract 63 0) %ymm1)
                               ((_ extract 63 0) %ymm1)
                               (concat #x00000000 ((_ extract 127 96) %ymm1)))
                             ((_ extract 63 0) TMP_BV_128_1))))
  (concat #x00000000000000000000000000000000
          (concat (vfmsub132_double ((_ extract 127 64) %ymm2)
                                    ((_ extract 127 64) %ymm1)
                                    ((_ extract 127 64) TMP_BV_128_0))
                  a!1)))

Information about memory reads:
  Value TMP_BV_128_0 (16 bytes)
    was read at address %rbx.
  Value TMP_BV_128_1 (16 bytes)
    was read at address %rbx.

sigfpe  : sigfpe
sigbus  : sigbus
sigsegv : sigsegv
