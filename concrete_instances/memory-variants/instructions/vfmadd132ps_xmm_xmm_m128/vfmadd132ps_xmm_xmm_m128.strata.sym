code: vfmadd132ps (%rax), %xmm2, %xmm1

  maybe read:      { %rax %xmm1 %xmm2 }
  must read:       { %rax %xmm1 %xmm2 }
  maybe write:     { %ymm1 }
  must write:      { %ymm1 }
  maybe undef:     { }
  must undef:      { }
  required flags:  { fma }

Opcode: vfmadd132ps_xmm_xmm_xmm Reason: 1
Opcode: vfmadd132ps_xmm_xmm_m128 Reason: 2
vfmadd132ps (%rax), %xmm2, %xmm1: Hindex0(10)
Opcode: vfmadd132ps_xmm_xmm_xmm Reason: 1
Opcode: vfmadd132ps_xmm_xmm_m128 Reason: 2
vfmadd132ps (%rax), %xmm2, %xmm1: Hindex0(10)
callq .move_128_064_xmm3_r12_r13: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
xorq %r13, %r9: Hindex1(9)
xorq %r12, %r8: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall : Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm1_r8_r9: Hindex8(9)
xorq %r8, %r12: Hindex1(9)
xorq %r9, %r13: Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r10_r11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
orq %r9, %r11: Hindex1(9)
orq %r8, %r10: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r12_r13: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
xorq %r13, %r9: Hindex1(9)
xorq %r12, %r8: Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r10_r11: Hindex8(9)
vzeroall : Hindex1(9)
callq .move_064_128_r10_r11_xmm3: Hindex8(9)
callq .move_256_128_ymm3_xmm12_xmm13: Hindex8(9)
callq .move_128_256_xmm12_xmm13_ymm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
vfnmsub132ps %ymm1, %ymm13, %ymm6: Hindex1(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r12_r13: Hindex8(9)
vzeroall : Hindex1(9)
callq .move_064_128_r12_r13_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
vfmsub132ps %ymm0, %ymm4, %ymm1: Hindex1(9)
callq .move_256_128_ymm3_xmm8_xmm9: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_064_xmm3_r10_r11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
orq %r9, %r11: Hindex1(9)
orq %r8, %r10: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_256_128_ymm2_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm3_r10_r11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
orq %r9, %r11: Hindex1(9)
orq %r8, %r10: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
callq .move_256_128_ymm3_xmm10_xmm11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
callq .move_064_128_r8_r9_xmm1: Hindex8(9)
callq .move_128_256_xmm8_xmm9_ymm1: Hindex8(9)
callq .move_128_064_xmm3_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
callq .move_064_128_r8_r9_xmm2: Hindex8(9)
callq .move_128_064_xmm3_r10_r11: Hindex8(9)
callq .move_128_064_xmm2_r8_r9: Hindex8(9)
vzeroall : Hindex1(9)
orq %r9, %r11: Hindex1(9)
orq %r8, %r10: Hindex1(9)
callq .move_064_128_r10_r11_xmm1: Hindex8(9)
Formula:

%ymm1  : 0x0₁₂₈ ∘ ((0x0₆₄ | (vfmsub132_single(%ymm1[127:96], vfnmsub132_single(0x0₃₂ ⊕ %ymm2[127:96] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm2[127:96]), %ymm2[127:96], %ymm2[127:96]), TMP_BV_128_0[127:96]) ∘ vfmsub132_single(%ymm1[95:64], vfnmsub132_single(0x0₃₂ ⊕ %ymm2[95:64] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm2[95:64]), %ymm2[95:64], %ymm2[95:64]), TMP_BV_128_1[95:64]) | 0x0₆₄ ⊕ %ymm2[127:64] ⊕ (0x0₆₄ | 0x0₆₄ ⊕ %ymm2[127:64]))) ∘ (0x0₆₄ | (vfmsub132_single(%ymm1[63:32], vfnmsub132_single(0x0₃₂ ⊕ %ymm2[63:32] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm2[63:32]), %ymm2[63:32], %ymm2[63:32]), TMP_BV_128_2[63:32]) ∘ vfmsub132_single(%ymm1[31:0], vfnmsub132_single(0x0₃₂ ⊕ %ymm2[31:0] ⊕ (0x0₃₂ | 0x0₃₂ ⊕ %ymm2[31:0]), %ymm2[31:0], %ymm2[31:0]), TMP_BV_128_3[31:0]) | 0x0₆₄ ⊕ %ymm2[63:0] ⊕ (0x0₆₄ | 0x0₆₄ ⊕ %ymm2[63:0]))))

Information about memory reads:
  Value TMP_BV_128_0 (16 bytes)
    was read at address %rax.
  Value TMP_BV_128_1 (16 bytes)
    was read at address %rax.
  Value TMP_BV_128_2 (16 bytes)
    was read at address %rax.
  Value TMP_BV_128_3 (16 bytes)
    was read at address %rax.

sigfpe  : sigfpe
sigbus  : sigbus
sigsegv : sigsegv
